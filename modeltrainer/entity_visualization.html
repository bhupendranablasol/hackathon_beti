<div class="entities" style="line-height: 2.5; direction: ltr">100G Networking Technology Overview<br>
<mark class="entity" style="background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Christopher Lameter
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">PERSON</span>
</mark>
 &lt;cl@linux.com&gt; <br>
<mark class="entity" style="background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Fernando Garcia
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">PERSON</span>
</mark>
 &lt;fgarcia@dasgunt.com&gt; <br>Toronto, August 23, 2016<br>1<br><br>Why 100G now?<br>•Capacity and speed requirements on data links keep increasing. <br>•Fiber link reuse in the Connectivity providers (Allows Telcos to <br>make better use of WAN links) <br>•Servers have begun to be capable of sustaining 100G to memory <br>(Intel Skylake, IBM Power8+) <br>•Machine Learning Algorithms require more bandwidth <br>•Exascale Vision for 2020 of the US DoE to move the industry <br>ahead.<br>2

100G Networking Technologies<br>•10 x 10G Link old standard CFP C??. Expensive. Lots of cabling. Has been in use for awhile for <br>specialized uses. <br>•New 4 x 28G link standards &quot;QSFP28&quot;. Brings down price to ranges of SFP and QSFP. <br>Compact and designed to replace 10G and 40G networking. <br>•Infiniband (EDR) <br>oStandard pushed by Mellanox. <br>oTransitioning to lower Infiniband speeds through switches. <br>oMost mature technology to date. Switches and NICs are available. <br>•Ethernet <br>oEarly deployment in 2015. <br>o But most widely used chipset for switches recalled to be respun.  <br>oNICs are under development. Mature one is the Mellanox EDR adapter that can run in 100G 
Ethernet mode. <br>oMaybe ready mid 2016. <br>•Omnipath (Intel) <br>•Redesigned serialization. No legacy issues with Infiniband. More nodes. Designed for Exascale <br>vision. Immature. Vendor claims production readiness but what is available has the character of <br>an alpha release with limited functionality. Estimate that this is going to be more mature at the 
end of 2016.<br>3<br><br>CFP vs QSFP28: 100G Connectors <br>4<br><br>Splitting 100G Ethernet to 25G and 50G
•100G is actually 4x25g (QSFP28), so 100G Ports can be split with <br>“octopus cables” to lower speed. <br>•50G (2x25) and 25G (1x25G) speeds are available which doubles or <br>quadruples the port density of switches. <br>•Some switches can handle 32 links of 100G, 64 of 50G and 128 of 25G. <br>•It was a late idea. So 25G Ethernet standards are scheduled to be <br>completed in 2016 only. Vendors are designing to a proposed standard. <br>•50G Ethernet standard is in the works (2018-2020). May be the default <br>in the future since storage speeds and memory speeds increase. <br>•100G Ethernet done <br>•25G Ethernet has a new connector standard called SFP28
5

100G Cabling and Connectors<br>6<br><br>100G Switches<br>7<br>Ports Status Name<br>Mellanox <br>Infiniband<br>EDR x 36 Released. Stable.7700 Series<br>Broadcom 100G x 32 <br>50G x 64 <br>25G x 128<br>Rereleased after <br>silicon problem.<br>Tomahawk Chip<br>Mellanox <br>Ethernet<br>100G x 32 <br>50G x 64<br>2Q ?	Spectrum<br>Intel Omnipath x 48 Available 100 Series<br><br>Overwhelmed by data<br>8
Ethernet	10M 100M (Fast)1G (Gigabit)10G 100G<br>Time per bit 100 ns 10 ns 1 ns 0.1 ns 0.01 ns<br>Time for a MTU size <br>frame 1500 bytes<br>1500 us 150 us 15 us 1.5 us 150 ns<br>Time for a 64 byte <br>packet<br>64 us 6.4 us 640 ns 64 ns 6.4 ns<br>Packets per second ~10 K ~100 K ~1 M ~10 M ~100 M<br>Packets per 10 us	2 (small) 20 <br>(small)<br>6 (MTU) 60 (MTU)<br><br>No time to process what you get?<br>9<br>•NICs have the problem of how to <br>get  the data to the application <br>•Flow Steering in the kernel allows <br>the distribution of packets to <br>multiple processors so that the <br>processing scales. But there are not <br>enough processing cores for 100G. <br>•NICs have extensive logic to offload <br>operations and distribute the load. <br>•One NIC supports multiple servers <br>of diverse architectures <br>simultaneously. <br>•Support for virtualization. SR-IOV <br>etc. <br>•Switch like logic on the chip.<br>1 us = 1 microsecond <br>= 1/1000000 seconds <br>1 ns = 1 nanosecond <br>= 1/1000 us <br>Network send or receive syscall: <br>10-20 us <br>Main memory access: <br>~100 ns<br><br>Available 100G NICs<br>•Mellanox ConnectX4 Adapter <br>•100G Ethernet <br>•EDR Infiniband <br>•Sophisticated offloads. <br>•Multi-Host <br>•Evolution of ConnectX3 <br>•Intel Omnipath Adapter <br>•Focus on MPI <br>•Omnipath only <br>•Redesign of IB protocol to be a “Fabric” <br>•“Fabric Adapter”. New Fabric APIs. <br>•More nodes larger transfer sizes<br>10<br><br>Application Interfaces and 100G
1.Socket API (Posix) <br>Run existing apps. Large code base. Large set of developers that know how to use the programming interface <br>2.Block level File I/O <br>Another POSIX API. Remote filesystems like NFS may use NFSoRDMA etc <br>3.RDMA API <br>1.One sided transfers <br>2.Receive/SendQ in user space <br>3.Talk directly to the hardware. <br>4.OFI <br>Fabric API designed for application interaction not with the network but the “Fabric” <br>5.DPDK <br>Low level access to NIC from user space.<br>11<br><br>Using the Socket APIs with 100G<br>•Problem of queuing if you have a fast talker. <br>•Flow steering to scale to multipe processors <br>•Per processor queues to scale sending. <br>•Exploit offloads to send / receive large amounts <br>of data <br>•May use protocol with congestion control (TCP) <br>but then not able to use full bandwidth. <br>Congestion control not tested with 100G. <br>•Restricted to Ethernet 100G. Use on non 
Ethernet Fabrics (IPoIB, IPoFabric) has various <br>non Ethernet semantics. F.e. Layer 2 behaves <br>differently and may offer up surprises.<br>12

RDMA / Infiniband API<br>•Allow use of native Infiniband functionality designed for higher speed. <br>•Supports both Infiniband and Onmipath. <br>•Single sided transfers via memory registration or classic messaging. <br>•Offload behavior by having RX and TX rings in user space. <br>•Group send / receive possible. <br>•Control of RDMA/Infiniband from user space with API that is process <br>safe but allows direct interaction with an instance of the NIC. <br>•Can be used on Ethernet via ROCE and/or ROCEv2 <br>•Generally traffic is not routable (ROCE V2 and Ethernet messaging of <br>course is). Problem of getting into and out of fabric. Requires <br>specialized gateways.<br>13<br><br>OFI (aka libfabric)<br>•Recent project by OFA to design a new API. <br>•Based on RDMA concepts. <br>•Driven by Intel to have an easier API than the ugly RDMA <br>APIs. OFI is focusing on the application needs from a Fabric. <br>•Tested and developed for the needs of MPI at this point. <br>•Ability to avoid the RDMA kernel API via “native” drivers. <br>Drivers can define API to their own user space libraries. <br>•Lack of some general functionality like Multicast. <br>•Immature at this point. Promise for the future to solve some <br>of the issue coming with 100G networking.<br>14<br><br>Software Support for 100G technology<br>EDR via Mellanox ConnectX4 Adapter <br> - Linux 4.3. Redhat 7.2 <br>Ethernet via Mellanox ConnectX4 Adapter <br> - Linux 4.5. Redhat 7.3. <br> (7.2 has only socket layer support). <br>Omnipath via Intel OPA adapter <br> - Out of tree driver s, in Linux 4.4 staging. <br>Currently supported via Intel OFED <br>distribution <br>15<br><br>Test Hardware<br>oIntel(R) Xeon(R) CPU E5-2667 v3 @ 3.20GHz <br>•Adapters <br>oIntel Omni-Path Host Fabric Interface Adapter <br>•Driver Version: 0.11-162 <br>•Opa Version: 10.1.1.0.9 <br>oMellanox ConnectX-4 VPI Adapter <br>•Driver Version: Stock RHEL 7.2 <br>•Firmware Version: 12.16.1020 <br>•Switches <br>oIntel 100 OPA Edge 48p <br>•Firmware Version: 10.1.0.0.133 <br>oMellanox SB7700 <br>•Firmware Version: 3.4.3050<br>16<br><br>Latency Tests via RDMA APIs(ib_send_lat)<br>17<br>Typical Latency (usec)<br>0.00<br>2.75<br>5.50<br>8.25<br>11.00<br>Msg Size (bytes)<br>248163264128256512102420484096      <br>EDR <br>Omnipath <br>100GbE <br>10GbE <br>1GbE<br>-Consistently low latency below 50% of 1G Ethernet. <br>-Higher packet sizes benefit significantly. <br>-Omnipath has higher latency due to software processing of send/receive <br>requests. <br><br>Bandwidth Tests using RDMA APIs (ib_send_bw)<br>18<br>BW average (MB/sec)<br>0.00<br>3000.00<br>6000.00<br>9000.00<br>12000.00<br>Msg Size (bytes)<br>248163264128256512102420484096      <br>EDR 
Omnipath <br>100GbE <br>10GbE <br>1GbE<br>-EDR can reach line saturation (~12GB/sec) at ~ 2k packet size <br>-Small packet processing is superior on EDR. <br>-10GE (1GB/sec) and 1G (120GB/sec) saturate the line with small packets <br>early<br><br>Multicast latency tests<br>19<br>Latency (us)<br>0<br>1<br>2<br>3<br>4
EDR Omnipath 100GbE 10GbE 1GbE<br>- Lowest latency with 100G and 10G Ethernet <br>-Slightly higher latency of EDR due to Forward Error Correction <br>-Software processing increases packet latency on Omnipath<br><br>RDMA vs. Posix Sockets (30 byte payload)<br>20<br>Latency (us)<br>0<br>4.5<br>9<br>13.5<br>18
EDR Omnipath 100GbE 10GbE 1GbE<br>Socket<br>RDMA<br><br>RDMA vs. Posix Sockets (1000 byte Payload)<br>21<br>Latency (us)<br>0<br>7.5<br>15<br>22.5<br>30
EDR Omnipath 100GbE 10GbE 1GbE<br>Sockets<br>RDMA<br><br>Further Reading Material<br>http://presentations.interop.com/events/las-vegas/2015/open-to-all---keynote-<br>presentations/download/2709 <br>https://en.wikipedia.org/wiki/100_Gigabit_Ethernet <br>http://www.ieee802.org/3/index.html <br>22<br><br>Memory Performance issues with 100G
•100G NIC can give you 12.5G Byte per second of <br>throughput <br>•DDR3 memory in basic configuration at 6.5 Gbyte <br>per sec. High end at 17G byte per second. <br>•DDR4 12.8G - 19.2G byte per sec. <br>•Some adapter have dual 100G connectors.  <br>•Memory via the NIC traffic may be able to saturate <br>the system. <br>23<br><br>Looking Ahead<br>•100G is maturing. <br>•200G available in 2017/2018. <br>•Terabit links by 2022. <br>•Software needs to mature. Especially the <br>OS network stack to handle these speeds. <br>•Issues <br>oMemory throughput <br>oProper APIs <br>oDeeper integration of cpu/memory/io<br>24<br><br>Q&amp;A<br>•Issues <br>•Getting involved <br>•How to scale the OS and software <br>•What impact will this speed have on software <br>•Contact information <br>  cl@linux.com<br>25</div>